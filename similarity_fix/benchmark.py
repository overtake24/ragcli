#!/usr/bin/env python3
# similarity_fix/benchmark.py

import json
import time
import argparse
import os
import sys
from datetime import datetime
from similarity_adapter import SimilarityAdapter


def run_benchmark(queries=None, metric="l2", strategy="hybrid", top_k=5):
    """
    Benzerlik adaptÃ¶rÃ¼nÃ¼ test etmek iÃ§in benchmark Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    if queries is None:
        queries = [
            "Marie Curie kimdir?",
            "Inception filmi hakkÄ±nda bilgi ver",
            "YÃ¼zÃ¼klerin Efendisi kitabÄ± nedir?"
        ]

    adapter = SimilarityAdapter(metric=metric, strategy=strategy)

    all_results = {}
    total_time = 0

    for query in queries:
        print(f"ğŸ” Sorgu: '{query}'")
        start_time = time.time()
        results = adapter.query(query, top_k=top_k)
        end_time = time.time()
        elapsed = end_time - start_time
        total_time += elapsed

        if not results:
            print(f"âš ï¸ Sorgu iÃ§in sonuÃ§ bulunamadÄ±: {query}")
            continue

        print(f"â±ï¸ Ã‡alÄ±ÅŸma sÃ¼resi: {elapsed:.6f} saniye")
        print(f"ğŸ“Š {len(results)} sonuÃ§ bulundu")

        for i, res in enumerate(results):
            title = res.get('title', 'BaÅŸlÄ±ksÄ±z')
            category = res.get('category', 'bilinmiyor')
            score = res.get('normalized_score', res.get('score', 0))
            print(f"  {i + 1}. {title} - Kategori: {category}, Benzerlik: {score:.4f}")

        query_results = {
            "query": query,
            "top_k": top_k,
            "elapsed_time": elapsed,
            "results": results
        }
        all_results[query] = query_results

    avg_time = total_time / len(queries) if queries else 0
    benchmark_results = {
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "metric": metric,
        "strategy": strategy,
        "top_k": top_k,
        "queries": queries,
        "avg_time": avg_time,
        "results": all_results
    }

    return benchmark_results


def main():
    parser = argparse.ArgumentParser(description="Benchmark iÃ§in RAG Benzerlik DÃ¼zeltme ModÃ¼lÃ¼")
    parser.add_argument("--output", type=str, default="benchmark_results.json", help="Benchmark sonuÃ§ dosya yolu")
    parser.add_argument("--queries", type=str, help="SorgularÄ± iÃ§eren dosya yolu")
    parser.add_argument("--metric", type=str, default="l2", choices=["l2", "cosine", "inner"],
                        help="KullanÄ±lacak benzerlik metriÄŸi")
    parser.add_argument("--strategy", type=str, default="hybrid", choices=["reverse", "scale", "hybrid"],
                        help="KullanÄ±lacak strateji")
    parser.add_argument("--top-k", type=int, default=5, help="Her sorgu iÃ§in dÃ¶necek sonuÃ§ sayÄ±sÄ±")
    args = parser.parse_args()

    queries = None
    if args.queries:
        try:
            with open(args.queries, "r", encoding="utf-8") as f:
                queries = [line.strip() for line in f if line.strip()]
        except Exception as e:
            print(f"âŒ Sorgu dosyasÄ± okuma hatasÄ±: {e}")
            return

    print(f"âš™ï¸ Benchmark Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    print(f"  Metrik: {args.metric}")
    print(f"  Strateji: {args.strategy}")
    print(f"  Top-K: {args.top_k}")

    results = run_benchmark(queries, args.metric, args.strategy, args.top_k)

    # SonuÃ§larÄ± dosyaya kaydet
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)

    print(f"âœ… Benchmark sonuÃ§larÄ± {args.output} dosyasÄ±na yazÄ±ldÄ±.")
    print(f"â±ï¸ Ortalama Ã§alÄ±ÅŸma sÃ¼resi: {results['avg_time']:.6f} saniye")


if __name__ == "__main__":
    main()