# app/llm.py
"""
LLM iÅŸlemleri.
"""
import json
import os
import re
from pydantic import create_model, Field
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain_core.documents import Document

from app.config import LLM_MODEL, MODEL_SCHEMA_FILE, PROMPT_TEMPLATE_FILE
from app.embedding import get_embeddings
from app.db import get_vectorstore


def get_llm():
    """
    LLM modelini dÃ¶ndÃ¼r.
    """
    return Ollama(model=LLM_MODEL)


def load_model_schema(schema_name="DocumentResponse", schema_file=MODEL_SCHEMA_FILE):
    """
    Dinamik model ÅŸablonunu yÃ¼kle.
    """
    if not os.path.exists(schema_file):
        os.makedirs(os.path.dirname(schema_file), exist_ok=True)
        default_schema = {
            "DocumentResponse": {
                "fields": {
                    "title": {"type": "str", "description": "Belgenin baÅŸlÄ±ÄŸÄ±"},
                    "summary": {"type": "str", "description": "Ä°Ã§erik Ã¶zeti"},
                    "key_points": {"type": "list[str]", "description": "Anahtar noktalar"}
                }
            }
        }
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(default_schema, f, indent=2, ensure_ascii=False)

    with open(schema_file, 'r', encoding='utf-8') as f:
        schemas = json.load(f)

    if schema_name not in schemas:
        raise ValueError(f"Åema '{schema_name}' bulunamadÄ±")

    schema_def = schemas[schema_name]
    fields = {}

    for field_name, field_props in schema_def["fields"].items():
        field_type = eval(field_props["type"])
        fields[field_name] = (field_type, Field(description=field_props["description"]))

    return create_model(schema_name, **fields)


def load_prompt_template(template_name="default", template_file=PROMPT_TEMPLATE_FILE):
    """
    Dinamik prompt ÅŸablonunu yÃ¼kle.
    """
    if not os.path.exists(template_file):
        os.makedirs(os.path.dirname(template_file), exist_ok=True)
        default_templates = {
            "default": {
                "messages": [
                    {"role": "system",
                     "content": "Sen bir uzman asistansÄ±n. KullanÄ±cÄ±nÄ±n sorgusunu, verilen baÄŸlamÄ± kullanarak yanÄ±tla."},
                    {"role": "user", "content": "Soru: {query}\nBaÄŸlam: {context}\nCevap:"}
                ]
            }
        }
        with open(template_file, 'w', encoding='utf-8') as f:
            json.dump(default_templates, f, indent=2, ensure_ascii=False)

    with open(template_file, 'r', encoding='utf-8') as f:
        templates = json.load(f)

    if template_name not in templates:
        raise ValueError(f"Åablon '{template_name}' bulunamadÄ±")

    template_def = templates[template_name]
    messages = [(msg["role"], msg["content"]) for msg in template_def["messages"]]

    return ChatPromptTemplate.from_messages(messages)


def create_rag_chain(template_name="default"):
    """
    LCEL kullanarak RAG zinciri oluÅŸtur.
    """
    llm = get_llm()
    db = get_vectorstore(get_embeddings())
    retriever = db.as_retriever(search_kwargs={"k": 3})
    prompt_template = load_prompt_template(template_name)

    # LCEL ile zincir oluÅŸtur
    return (
            {"query": RunnablePassthrough(), "context": retriever}
            | prompt_template
            | llm
            | StrOutputParser()
    )


# app/llm.py dosyasÄ±nda parse_structured_data fonksiyonu bu ÅŸekilde deÄŸiÅŸtirilmeli:

def parse_structured_data(question, context, model_name, template_name, sources):
    """
    YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri modelleri iÃ§in metinsel verileri analiz eder.

    Args:
        question: KullanÄ±cÄ± sorusu
        context: BaÄŸlam metni
        model_name: KullanÄ±lacak model adÄ± (FilmInfo, BookInfo, PersonInfo vb.)
        template_name: KullanÄ±lacak ÅŸablon adÄ± (film_query, book_query, person_query vb.)
        sources: Kaynak belgeler

    Returns:
        YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri nesnesi ve kullanÄ±lan kaynaklar
    """
    print(f"INFO - YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri sorgusu algÄ±landÄ±: {model_name} modeli ile iÅŸleniyor...")

    # Prompt hazÄ±rlama
    prompt = f"""Sen bir veri ayrÄ±ÅŸtÄ±rma uzmanÄ±sÄ±n. Verilen metni analiz ederek ilgili tÃ¼m bilgileri Ã§Ä±kar ve JSON formatÄ±nda yapÄ±landÄ±r.

Soru: {question}

Ä°lgili belgeler:
{context}

Metinden tÃ¼m Ã¶nemli bilgileri Ã§Ä±kar ve aÅŸaÄŸÄ±daki JSON formatÄ±nda dÃ¶ndÃ¼r:

"""

    # Model tipine gÃ¶re ÅŸablon ekleme
    if model_name == "FilmInfo":
        prompt += """{
  "title": "Filmin baÅŸlÄ±ÄŸÄ±",
  "plot_summary": "Film Ã¶zeti",
  "cast": [
    {
      "name": "Oyuncu adÄ±",
      "role": "OynadÄ±ÄŸÄ± karakter"
    }
  ],
  "director": "YÃ¶netmen adÄ±",
  "genre": ["TÃ¼r1", "TÃ¼r2"],
  "release_year": "Ã‡Ä±kÄ±ÅŸ yÄ±lÄ±",
  "imdb_rating": "IMDb puanÄ±"
}"""
    elif model_name == "BookInfo":
        prompt += """{
  "title": "Kitap baÅŸlÄ±ÄŸÄ±",
  "author": "Yazar adÄ±",
  "summary": "Kitap Ã¶zeti",
  "genre": ["TÃ¼r1", "TÃ¼r2"],
  "publish_year": "YayÄ±n yÄ±lÄ±",
  "page_count": "Sayfa sayÄ±sÄ±",
  "rating": "Puanlama"
}"""
    elif model_name == "PersonInfo":
        prompt += """{
  "name": "KiÅŸinin adÄ±",
  "birth_date": "DoÄŸum tarihi",
  "death_date": "Ã–lÃ¼m tarihi (varsa)",
  "nationality": "Uyruk",
  "occupation": ["Meslek1", "Meslek2"],
  "biography": "KÄ±sa biyografi",
  "notable_works": ["Eser1", "Eser2"]
}"""
    else:
        # Genel yapÄ±landÄ±rÄ±lmÄ±ÅŸ veri formatÄ±
        prompt += f"Model iÃ§in uygun JSON formatÄ±nÄ± kullan ({model_name})"

    prompt += "\n\nSadece JSON dÃ¶ndÃ¼r, baÅŸka aÃ§Ä±klama ekleme. EÄŸer belirli bir bilgiyi bulamazsan, ilgili alanÄ± boÅŸ bÄ±rak veya \"\" kullan, null kullanma."

    # LLM Ã§aÄŸrÄ±sÄ±
    llm = get_llm()
    raw_answer = llm(prompt)

    print(f"DEBUG - YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri LLM yanÄ±tÄ±: {raw_answer[:200]}...")

    # JSON formatÄ±nÄ± Ã§Ä±kar
    try:
        # Kod bloÄŸu iÅŸaretlerini kaldÄ±r (```json ve ```)
        cleaned_json = raw_answer
        if "```" in cleaned_json:
            # Sadece iÃ§eriÄŸi al
            code_block_pattern = r"```(?:json)?\s*([\s\S]*?)```"
            matches = re.findall(code_block_pattern, cleaned_json)
            if matches:
                cleaned_json = matches[0].strip()

        # JSON'Ä± parse et
        structured_data = json.loads(cleaned_json)

        # Null deÄŸerleri varsayÄ±lan deÄŸerlerle deÄŸiÅŸtir
        for key in structured_data:
            if structured_data[key] is None:
                # Liste tipleri iÃ§in boÅŸ liste
                if key in ["occupation", "notable_works", "genre", "cast", "key_points"]:
                    structured_data[key] = []
                # String tÃ¼rleri iÃ§in boÅŸ string
                else:
                    structured_data[key] = ""

        # Model ÅŸablonunu yÃ¼kle
        model_schema = load_model_schema(model_name)

        # Modele gÃ¶re dÃ¶nÃ¼ÅŸÃ¼m yap
        try:
            result = model_schema(**structured_data)
            return result, sources
        except Exception as e:
            print(f"HATA: YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri modele dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼rken hata: {e}")
            # VarsayÄ±lan deÄŸerlerle nesne oluÅŸtur
            default_values = {}
            for field_name in model_schema.__annotations__:
                if field_name in ["occupation", "notable_works", "genre", "cast", "key_points"]:
                    default_values[field_name] = []
                else:
                    default_values[field_name] = "Bilgi bulunamadÄ±"

            # Bulunan deÄŸerleri ekle
            for key, value in structured_data.items():
                if key in default_values and value not in [None, ""]:
                    default_values[key] = value

            return model_schema(**default_values), sources

    except Exception as e:
        print(f"HATA: YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {e}")

        # BoÅŸ bir ÅŸablon nesne dÃ¶ndÃ¼r - varsayÄ±lan deÄŸerlerle
        empty_schema = load_model_schema(model_name)
        default_values = {}
        for field_name in empty_schema.__annotations__:
            if field_name in ["occupation", "notable_works", "genre", "cast", "key_points"]:
                default_values[field_name] = []
            else:
                default_values[field_name] = "Bilgi bulunamadÄ±"

        return empty_schema(**default_values), sources


# Ä°lgili llm.py bÃ¶lÃ¼mÃ¼ gÃ¼ncellendi - veritabanÄ± sorgulama ve sorgu filtreleme

def query(question, template_name="default", model_name="DocumentResponse", embedding_model=None):
    """
    Sorgu yap ve yanÄ±tÄ± dÃ¶ndÃ¼r.
    """
    from app.config import EMBEDDING_MODEL, SIMILARITY_THRESHOLD, MAX_DOCUMENTS, DOCUMENT_CATEGORIES
    if embedding_model is None:
        embedding_model = EMBEDDING_MODEL

    print(f"INFO - Sorgu embedding modeli: {embedding_model}")
    embeddings = get_embeddings(embedding_model)
    db = get_vectorstore(embeddings)

    # VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± ve tablolarÄ± kontrol et
    print("DEBUG - VeritabanÄ± kontrol ediliyor")
    docs = []
    docs_with_scores = []  # Skorlarla birlikte belgeleri sakla

    try:
        # Kategori bazlÄ± filtreleme iÃ§in sorgu analizi
        query_category = detect_query_category(question)
        print(f"INFO - AlgÄ±lanan sorgu kategorisi: {query_category}")

        # Benzerlik skorlarÄ± ile birlikte belgeleri getir
        try:
            # similarity_search_with_score kullanarak benzerlik skorlarÄ±nÄ± al
            docs_with_scores = db.similarity_search_with_score(question,
                                                               k=MAX_DOCUMENTS * 2)  # Daha fazla belge getir, sonra filtreleyeceÄŸiz

            # Belgelerin benzerlik skorlarÄ±nÄ± gÃ¶ster
            print(f"\nğŸ” '{question}' sorgusu iÃ§in benzerlik skorlarÄ±:")
            print("=" * 50)
            for i, (doc, score) in enumerate(docs_with_scores):
                source = doc.metadata.get('source', 'bilinmiyor')
                # KosinÃ¼s benzerliÄŸi genellikle [0-1] aralÄ±ÄŸÄ±nda olur, 1 en yÃ¼ksek benzerlik
                # BazÄ± implementasyonlarda L2 mesafesi kullanÄ±lÄ±r, bu durumda dÃ¼ÅŸÃ¼k deÄŸerler daha iyi benzerliÄŸi gÃ¶sterir
                # Formata gÃ¶re skoru ayarla
                score_display = score if score <= 1.0 else f"{1.0 / score:.4f}"
                similarity_percent = float(score_display) * 100 if score <= 1.0 else float(1.0 / score) * 100

                print(f"Belge {i + 1}: {source} - Benzerlik: {score_display} ({similarity_percent:.2f}%)")
                content_preview = doc.page_content[:100].replace('\n', ' ')
                print(f"  Ä°Ã§erik: {content_preview}...")

            # Filtreleme iÅŸlemleri
            filtered_docs_with_scores = []

            # 1. Benzerlik eÅŸiÄŸi filtresi
            for doc, score in docs_with_scores:
                # Skor formatÄ±na gÃ¶re kontrol et
                score_value = score if score <= 1.0 else 1.0 / score
                if score_value >= SIMILARITY_THRESHOLD:
                    filtered_docs_with_scores.append((doc, score))
                else:
                    print(
                        f"âš ï¸ DÃ¼ÅŸÃ¼k benzerlik skoru ({score_value:.4f}) nedeniyle filtrelendi: {doc.metadata.get('source')}")

            # 2. Kategori filtresi (film, kitap, kiÅŸi vb.)
            if query_category and query_category != "other":
                category_docs = []
                for doc, score in filtered_docs_with_scores:
                    doc_category = detect_document_category(doc.page_content)
                    if doc_category == query_category:
                        category_docs.append((doc, score))

                # EÄŸer kategori filtrelemesi sonucunda belge kaldÄ±ysa, sadece onlarÄ± kullan
                if category_docs:
                    print(
                        f"ğŸ“Œ '{query_category}' kategorisine gÃ¶re filtreleme yapÄ±ldÄ±: {len(category_docs)}/{len(filtered_docs_with_scores)} belge")
                    filtered_docs_with_scores = category_docs

            # Son olarak en benzer MAX_DOCUMENTS belge ile devam et
            filtered_docs_with_scores = sorted(filtered_docs_with_scores,
                                               key=lambda x: x[1] if x[1] <= 1.0 else 1.0 / x[1], reverse=True)[
                                        :MAX_DOCUMENTS]

            # Sadece belgeleri docs listesine ekle
            docs = [doc for doc, _ in filtered_docs_with_scores]

            print(f"ğŸ“Š Filtreleme sonrasÄ± {len(docs)} belge kaldÄ±")

        except Exception as e:
            print(f"Benzerlik skorlarÄ± ile belge getirme hatasÄ±: {e}")

            # Backup olarak standart sorgu yÃ¶ntemini dene
            try:
                retriever = db.as_retriever(search_kwargs={"k": MAX_DOCUMENTS})
                docs = retriever.get_relevant_documents(question)
                print(f"Standart retriever ile {len(docs)} belge bulundu")
            except Exception as e2:
                print(f"Standart retriever hatasÄ±: {e2}")

        # VeritabanÄ± hatasÄ± durumunda yerel dosyalardan belgeleri yÃ¼kleme iÅŸlemi devam ediyor...
        if not docs:
            try:
                # Tablo kontrolÃ¼ ve yerel dosya yÃ¼kleme iÅŸlemleri... (mevcut kod devam ediyor)
                pass
            except Exception as e:
                print(f"DEBUG - VeritabanÄ± hatasÄ±: {e}")

    except Exception as e:
        print(f"DEBUG - Genel sorgu hatasÄ±: {e}")

    print(f"DEBUG - Sorgu: {question}")
    print(f"DEBUG - Toplam {len(docs)} belge getirildi")

    # Belgeleri formatla
    context = ""
    for i, doc in enumerate(docs):
        source = doc.metadata.get("source", f"Belge {i + 1}")
        context += f"[BELGE {i + 1}] {source}:\n{doc.page_content}\n\n"

    if not docs:
        context = "HiÃ§ ilgili belge bulunamadÄ±."

    # Kaynak iÃ§erik Ã¶zetleri ve benzerlik skorlarÄ±
    sources = []
    for i, doc in enumerate(docs):
        doc_source = doc.page_content[:100] + "..."
        # Benzerlik skoru varsa ekle
        if i < len(filtered_docs_with_scores):
            score = filtered_docs_with_scores[i][1]
            score_display = score if score <= 1.0 else f"{1.0 / score:.4f}"
            doc_source = f"{doc_source} [Benzerlik: {score_display}]"
        sources.append(doc_source)

    # YapÄ±landÄ±rÄ±lmÄ±ÅŸ veri modelleri iÃ§in Ã¶zel iÅŸleme
    if model_name in ["FilmInfo", "BookInfo", "PersonInfo"] or template_name in ["film_query", "book_query",
                                                                                 "person_query", "structured_data"]:
        return parse_structured_data(question, context, model_name, template_name, sources)

    # DiÄŸer iÅŸlemler aynen devam ediyor...
    # LLM Ã§aÄŸrÄ±sÄ± ve yanÄ±t iÅŸleme kodlarÄ±

    # Rest of the function remains the same...


# llm.py dosyasÄ±na SADECE BUNLARI ekleyin
# FonksiyonlarÄ±n tanÄ±mlarÄ± (en sonda olacak ÅŸekilde)

def detect_query_category(query):
    """
    Sorgu metnine gÃ¶re hangi kategoriyle ilgili olduÄŸunu tespit eder
    """
    query_lower = query.lower()

    # Film/dizi kategorisi
    film_keywords = ["film", "movie", "sinema", "cinema", "yÃ¶netmen", "director",
                     "oyuncu", "actor", "izle", "watch", "imdb", "Ã§ekim", "shooting"]

    # Kitap kategorisi
    book_keywords = ["kitap", "book", "yazar", "author", "eser", "roman", "novel",
                     "sayfa", "page", "okuma", "reading", "basÄ±m", "publication"]

    # KiÅŸi/biyografi kategorisi
    person_keywords = ["kim", "who", "kiÅŸi", "person", "doÄŸum", "birth", "Ã¶lÃ¼m", "death",
                       "hayat", "life", "ne zaman", "when", "meslek", "occupation", "biyografi", "biography"]

    # Kategori belirle
    film_score = sum(1 for word in film_keywords if word in query_lower)
    book_score = sum(1 for word in book_keywords if word in query_lower)
    person_score = sum(1 for word in person_keywords if word in query_lower)

    # En yÃ¼ksek skora sahip kategoriyi dÃ¶ndÃ¼r
    if film_score > book_score and film_score > person_score:
        return "film"
    elif book_score > film_score and book_score > person_score:
        return "book"
    elif person_score > film_score and person_score > book_score:
        return "person"

    # Belirsizse "other" dÃ¶ndÃ¼r
    return "other"


def filter_documents_by_category(docs, category):
    """
    Belgeleri kategoriye gÃ¶re filtreler
    """
    if category == "other":
        return docs

    # Kategori anahtar kelimeleri
    category_keywords = {
        "film": ["film", "movie", "sinema", "cinema", "yÃ¶netmen", "director", "cast", "oyuncu",
                 "imdb", "actor", "izle", "watch", "Ã§ekim", "shooting"],
        "book": ["kitap", "book", "yazar", "author", "eser", "roman", "novel", "sayfa",
                 "page", "okuma", "reading", "basÄ±m", "publication"],
        "person": ["kiÅŸi", "person", "doÄŸum", "birth", "Ã¶lÃ¼m", "death", "hayat", "life",
                   "yaÅŸam", "meslek", "occupation", "biyografi", "biography"]
    }

    # SeÃ§ilen kategorinin anahtar kelimeleri
    keywords = category_keywords.get(category, [])

    # Ä°lgili belgeleri filtrele
    filtered_docs = []
    for doc in docs:
        doc_text = doc.page_content.lower()
        # En az 2 anahtar kelime eÅŸleÅŸmesi olan belgeleri seÃ§
        keyword_matches = sum(1 for keyword in keywords if keyword in doc_text)
        if keyword_matches >= 2:
            filtered_docs.append(doc)

    # EÄŸer hiÃ§ belge kalmadÄ±ysa orijinal liste ile devam et
    if not filtered_docs and docs:
        print(f"âš ï¸ '{category}' kategorisi iÃ§in uygun belge bulunamadÄ±, tÃ¼m belgeler kullanÄ±lÄ±yor")
        return docs

    print(f"â„¹ï¸ '{category}' kategorisine gÃ¶re {len(filtered_docs)}/{len(docs)} belge filtrelendi")
    return filtered_docs